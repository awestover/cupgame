\documentclass[twocolumn]{article}[11pt]
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}

\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{xcolor}

\newcommand{\defn}[1]{{\textit{\textbf{\boldmath #1}}}}
\renewcommand{\paragraph}[1]{\vspace{0.09in}\noindent{\bf \boldmath #1.}} 
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\img}{Im}
\DeclareMathOperator{\polylog}{\text{polylog}}
\DeclareMathOperator{\poly}{\text{poly}}
\DeclareMathOperator{\st}{\text{ such that }}
\DeclareMathOperator{\tilt}{\text{tilt}}
\DeclareMathOperator{\fil}{\text{fill}}
\DeclareMathOperator{\avgfil}{\text{avgfill}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\contr}[0]{\[ \Rightarrow\!\Leftarrow \]}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newtheorem{fact}{Fact}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{clm}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}

\title{Variable-Processor Cup Games}
\author{Alek Westover}

\begin{document}
\maketitle

\section{Introduction}
The cup game is a classic game in computer science that models work-scheduling.
In the cup game a filler and an emptier take turns adding and removing water
(i.e. work) to the cups. We investigate a variant of the vanilla multiprocessor
cup game which we call the \defn{variable-processor cup game} in which the
filler is allowed to change the number of processors $p$ (the amount of water
that the filler can add and the number of cups from which the emptier can
remove water. This is a natural extension of the vanilla multi-processor cup
game to when the resources available are variable. Note that although the
restriction that the filler and emptier's resources vary together may seem
artificial, this is the only way to conduct the analysis; the rationale behind
giving the emptier and filler equal resources in the classical vanilla
multi-processor cup game is that this is the only way to achieve upper and
lower bounds. The equivalent rational holds for the motivation of the
variable-processor cup game. Analysis of this game does provide information
about how real-world systems will behave.

A priori the fact that the number of processors can vary offers neither the
filler nor the emptier a clear advantage: lower values of $p$ mean that the
emptier is at more of a discretization disadvantage but also mean that the
filler can anchor fewer cups.  We hoped that the variable-processor cup game
could be simulated in the vanilla multiprocessor cup game, because the extra
ability given to the filler does not seem very strong. The new version of the
cup game arose as we tried to get a bound of $\Omega(\log p)$ backlog in the
multiprocessor game against an oblivious filler, which would combine with
previous results to give us a lower bound that matches our upper bound:
$O(\log\log n + \log p)$. In Proposition \ref{prop:obliviousBase} we prove that
there is an oblivious filling strategy in the variable-processor cup game on
$n$ cups that achieve backlog $\Omega(\log n)$ as desired. \footnote{Note that
  we have $\Omega(\log n)$ in this proposition instead of $\Omega(\log p)$
  because the filler can increase the number of processors, so it increases the
  number of processors to $n-1$ to start. A nearly identical construction could
  be used to show that backlog $\Omega(\log p_{\max})$ can be achieved, where
  the number of processors starts at $p_{\max}$ and the filler does not ever
  increase the number of processors. However, using $p_{\max} = n$ is natural
  in the variable-processor cup game, so we do not consider the game with 
the restriction that the filler can not increase the number of processors above
some $p_{\max} < n$.}

However, we also show that attempts at simulating the variable-processor cup
game are futile because the variable-processor cup game
is--surprisingly--fundamentally different from the multiprocessor cup game, and
thus impossible to simulate. This follows as a corollary of an
\defn{Amplification Lemma} for both the adaptive and oblivious filler.
Section \ref{sec:adaptive} and Section \ref{sec:oblivious} follow the structure:
\begin{enumerate}
  \item Proposition: Base case of inductive argument in corollary
  \item Lemma: Amplification Lemma, allows for inductive step in inductive argument
  \item Corollary: Repeatedly amplify the base case backlog to get very large backlog
\end{enumerate}
We proceed with our results.

\section{Adaptive Lower Bound}\label{sec:adaptive}
\begin{proposition}
\label{prop:adaptiveBase}
  There exists an adaptive filling strategy for the variable-processor cup game
  on $n$ cups that achieves backlog at least $\frac{1}{4}\ln (n/2)$, where fill
  is relative to the average fill of the cups, with negative fill allowed.
\end{proposition}
\begin{proof}
  Let $h = \frac{1}{4}\ln (n/2)$ be the desired fill. Once a cup with fill at
  least $h$ is achieved the filler stops, the process completed. Denote the
  fill of a cup $i$ by $\fil(i)$. Let the \defn{positive tilt} of a cup $i$ be
  $\tilt_+(i) = \max(0, \fil(i))$, and let the positive tilt of a set $S$ of
  cups be $\sum_{i\in S} \tilt_+(i)$. Let the \defn{mass} of a set of cups $S$
  be $\sum_{i\in S} \fil(i)$. Let $A$ consist of the $n/2$ fullest cups, and
  $B$ consist of the rest of the cups.

  If the process is not yet complete, that is $\fil(i) < h$ for all cups $i$,
  then $\tilt_+(A\cup B) < h\cdot n$. Assume for
  sake of contradiction that there are more than $n/2$ cups $i$ with $\fil(i)
  \le -2h$. The mass of those cups would be less than $-hn $, but there isn't
  enough positive tilt to oppose this, a contradiction. Hence there are at most
  $n/2$ cups $i$ with $\fil(i) \le -2h$. 

  We set the number of processors equal to $1$ and play a single processor cup
  game on $n/2$ cups that have fill at least $-2h$ (which must exist) for $n/2
  -1$ steps. We initialize our ``active set" to be $A$, noting that $\fil(i)
  \ge -2h$ for all cups $i\in A$, and remove $1$ cup from the active set at
  each step.
  At each step the filler distributes water equally among the cups in its
  active set. Then, the emptier will choose some cup to empty from. If this cup
  is in the active set the filler removes it from the active set. Otherwise, the
  filler chooses an arbitrary cup to remove from the active set.

  After $n/2-1$ steps the active set will consist of a single cup. This cup's
  fill has increased by $1/(n/2) + 1/(n/2 - 1) + \cdots + 1/2 + 1/1
  \ge \ln n/2 = 4h$. Thus such a cup has fill at least $2h$ now, so the
  proposition is satisfied.
\end{proof}

\begin{lemma}[The Adaptive Amplification Lemma]\label{lem:adaptiveAmplification}
  Let $f$ be an adaptive filling strategy that achieves backlog $f(n)$ in the
  variable-processor cup game on $n$ cups (relative to average fill, with
  negative fill allowed).
  Let $n_0 \in \mathbb{N}$ be a constant such that $\frac{1}{4} \ln (n_0/2) \ge
  1$. Let $\delta\in(0,1)$ be constant, and let $M\in\mathbb{N}$ be a constant
  such that $n_0 \le (1-\delta)\delta^M n \le n_0/\delta$.

  Then, there exists an adaptive filling strategy that achieves \defn{amplified} fill
  $$f'(n) \ge \max\left(1, (1-\delta)\sum_{\ell= 0}^{M} f((1-\delta)\delta^\ell n)\right)$$
  on $n\ge n_0$ cups.
\end{lemma}
\begin{proof}
  The basic idea of this analysis is as follows:
  \begin{enumerate}
    \item Using $f$ repeatedly, achieve average fill at least $(1-\delta) f(n(1-\delta))$ in a set of $n\delta$ cups. 
    \item Reduce the number of processors to $n\delta$.
    \item Recurse on the $n\delta$ cups with high average fill.
  \end{enumerate}

  Let $A$ the \defn{anchor set} be initialized to consist of the $n\delta$
  fullest cups, and let $B$ the \defn{non-anchor set} be initialized to consist
  of the rest of the cups (so $|B| = (1-\delta)n$).
  Let $n_\ell = n\delta^{\ell-1}$, $h_\ell = (1-\delta)f(n_\ell(1-\delta))$;
  the filler will achieve a set of at least $n_\ell \delta$ cups with average
  fill at least $h_\ell$ on the $\ell$-th
  level of recursion. On the $\ell$-th level of recursion $|A| = \delta\cdot
  n_\ell, |B| = (1-\delta)\cdot n_\ell$.

  We now elaborate on how to achieve Step 1.
  Our filling strategy always places $1$ unit of water in each anchor cup. This
  ensures that average fill in the anchor set is non-decreasing.

  On the $\ell$-th level of recursion we will repeatedly apply $f$ to
  $B$, and then take the cup generated by $f$ within $B$ to have large backlog
  and swap it with a cup in $A$ until $A$ has the desired average fill. 
  Note that $$\avgfil(A) \cdot |A| +\avgfil(B)\cdot |B| = 0$$
  so $$\avgfil(A) = - \avgfil(B) \cdot (1-\delta)/ \delta.$$
  Thus, if at any point in this process $B$ has average fill lower than $-h_\ell
  \cdot \delta/(1-\delta)$,
  then anchor set has average fill at least $h_\ell$, so the process is
  finished. So long as $B$ has average fill at least $-h_\ell\cdot
  \delta/(1-\delta)$ we will apply $f$ to $B$.
  
  It is somewhat complicated to apply $f$ to $B$ however, because we need to
  guarantee that in the steps that the algorithm takes while applying $f$ the
  emptier always empties the same amount of water from $B$ as the filler fills
  $B$ with. This might not be the case if the emptier does not empty from each
  anchor cup at each step. Say that the emptier \defn{neglects} the anchor set
  on an application of $f$ if there is some step during the application of $f$
  in which the emptier does not empty from some anchor cup.

  We will apply $f$ to $B$ at most $h_\ell n_\ell\delta + 1$ times, and at the
  end of an application of $f$ we only swap the generated cup into $A$ if the
  emptier has not neglected the anchor set during the application of $f$.

  Note that each time the emptier neglects the anchor set the mass of the
  anchor set increases by $1$. If the emptier neglects the anchor set $h_\ell
  n_\ell\delta + 1$ times, then the average fill in the anchor set increases by
  more than $h_\ell$, so the desired average fill is achieved in the anchor set.

  Otherwise, there must have been an application of $f$ for which the emptier
  did not neglect the anchor set. We only swap a cup into the anchor set if
  this is the case. In this case we achieve fill 
  $$-h_\ell \cdot \delta/(1-\delta) + f(n_\ell (1-\delta)) = (1-\delta)f(n_\ell
  (1-\delta)) = h_\ell$$
  in a non-anchor cup, and swap it with the smallest cup in the anchor set.

  We achieve average fill $h_\ell$ in the anchor set for $M$ levels of
  recursion. Note that as $n\ge n_0$ we can allways simply use Proposition
  \ref{prop:adaptiveBase} to achieve backlog $1$. We will revert to this option
  if it gives larger fill than we get by repeatedly applying $f$.
\end{proof}

\begin{corollary}
  \label{cor:adaptivePoly}
  There is an adaptive filling strategy for the variable-processor cup game on
  $n$ cups that achieves backlog $\Omega(n^{1-\epsilon})$ for any constant
  $\epsilon \in (0,1)$, in running time $2^{O(\log^2 n)}$.
\end{corollary}
\begin{proof}$ $\\
  \paragraph{Basic Idea}
  Let
  $$f_0(k) = 
  \begin{cases} 
    \log_2 k, & k\geq 1, \\
    0 & \text{else.}
  \end{cases}$$
  Note that we can achieve backlog $f_0(k)$ on $k$ cups by Proposition \ref{prop:adaptiveBase}.
  Let $f_{m+1} $ be the result of applying the Amplification Lemma to $f_m$ with $\delta=1/2$. 
  The function $f_{\log_2 n^{1/9}}(k)$ satisfies 
  \begin{equation}\label{eqn:amppppp}
  \text{for } k \geq n,\,\, f_{\log_2 n^{1/9}}(k) \geq 2^{\log_2 n^{1/9}} \log_2 k.
  \end{equation}
  In particular, using $f_{\log_2 n^{1/9}}(n)$ (applying the function to all of
  the cups) we achieve backlog $\Omega(n^{1/9}\log_2 n) \ge \Omega(\poly(n))$
  as desired.
  To prove Equation \ref{eqn:amppppp}, we prove the following lower bound for $f_m$ by induction:
  $$f_m(k) \geq 2^m \log_2 k, \text{ for } k \geq (2^9)^m.$$
  The base case follows from the definition of $f_0$. Assuming the property for
  $f_m$, we get the following by Lemma \ref{lem:adaptiveAmplification}:
  $ \text{for } k > (2^9)^{m+1},$
  \begin{align*}
  &f_{m+1}(k) \\
  &\ge \frac{1}{2}(f_m(k/2) + f_m(k/4) + \cdots + f_m(k/2^9) + \cdots)\\
  &\geq \frac{1}{2}(f_m(k/2) + f_m(k/4) + \cdots + f_m(k/2^9))\\
  &\geq \frac{1}{2}2^m(\log_2 (k/2) + \log_2(k/4) + \cdots + \log_2(k/2^9))\\
  &\geq \frac{1}{2}2^m(9\log_2 (k) - \frac{9 \cdot 10}{2}) \\
  &\geq 2^{m+1} \log_2(k) ,
  \end{align*}
  as desired. Hence the inductive claim holds, which establishes that $f_{\log_2
  n^{1/9}}$ satisfies the desired condition, which proves that backlog can be
  made $\Omega(\poly(n))$.

  \paragraph{Running Time Analysis}
  The recursive construction requires quite a lot of steps, in fact a
  super-polynomial number of steps. If we consider the tree that represents
  computation of $f_{\log n^{1/\alpha}}(n)$ we see that each node will have at
  most $\alpha$ (some constant, e.g. $\alpha = 9$, $\alpha$ is the number of
  terms that we keep in the sum) children (the children of $f_k(c)$ are
  $f_{k-1}(c/2), f_{k-1}(c/4), \ldots f_{k-1}(c/2^\alpha)$), and the depth of
  the tree is $\log n^{1/\alpha}$. Say that the running time at the node
  $f_{\log n^{1/\alpha}}(n)$ is $T(n)$. Then because $f_{k}(n)$ must call each
  of $f_{k-1}(n/2^i)$ $n/2^i$ times for $1\le i \le \alpha$, we have that $
  T(n) \le \frac{\alpha n}{2}T(n/2)$. This recurrence yields $T(n) \le
  \poly(n)^{\log n} = O(2^{\log^2 n})$ for the running time.

  \paragraph{Generalizing Our Approach}
  Generalizing our approach we can achieve a (slightly) better polynomial
  lower bound on backlog. In our construction the point after which we had a
  bound for $f_m$ grew further out by a factor of $2^9$ each time. Instead of
  $2^9$ we now use $2^\alpha$ for some $\alpha \in \mathbb{N}$, and can find a
  better value of $\alpha$. The value of $\alpha$ dictates how many
  iterations we can perform: we can perform $\log_2 n^{1/\alpha}$ iterations.
  The parameter $\alpha$ also dictates the multiplicative factor that we gain
  upon going from $f_m$ to $f_{m+1}$. For $\alpha = 9$ this was $2$. In general
  it turns out to be $\frac{\alpha -1}{4}$.  Hence, we can achieve backlog
  $\Omega\left(\left(\frac{\alpha -1}{4}\right)^{\log_2 n^{1/\alpha}}\log_2
  n\right)$. This optimizes at $\alpha = 13$, to backlog
  $\Omega(n^{\frac{\log_2 3}{13}}\log n) \approx \Omega(n^{0.122}\log n)$. 

  We can further improve over this. Note that in the proof that
  $f_{m+1}$ gains a factor of $2$ over $f_m$ given above, we lower bound
  $9\log_2 k - 9\cdot 10 /2$ with $2\log_2 k$. Usually however this is very
  loose: for small $m$ a significant portion of the $9 \log_2 k$ is annihilated
  by the constant $1+2+\cdots+9$ (or in general $\alpha \log_2 k$ and
  $1+2+\cdots + \alpha$), but for larger values of $m$ because $k$ must be
  large we can get larger factors between steps, in theory factors arbitrarily
  close to $\alpha$. If we could gain a factor of $\alpha$ at each step, then
  the backlog achievable would be $\Omega(\alpha^{\log_2{n^{1/\alpha}}}\log n)=
  \Omega(n^{(\log_2{\alpha})/\alpha} \log n)$ which optimizes (over the
  naturals)
  at $\alpha = 3$ to $n^{(\log_2 3)/3} \approx n^{0.528}$. However, we can't
  actually gain a factor of $\alpha$ each time because of the subtracted
  constant. But, for any $\epsilon >0$ we can achieve a $\alpha - \epsilon$
  factor increase each time (for sufficiently large $m$). Of course $\epsilon$
  can't be made arbitrarily small because $m$ can't be made arbitrarily large,
  and the ``cut off" $m$ where we start achieving the $\alpha - \epsilon$
  factor increase must be a constant (not dependent on $n$). When the cutoff
  $m$, or equivalently $\epsilon$, is constant then we can achieve backlog
  $\Omega((\alpha - \epsilon)^{\log_2{n^{1/\alpha}}}\log n)=
  \Omega(n^{(\log_2(\alpha - \epsilon))/\alpha} \log n)$. For instance, with
  this method we can get backlog $\Omega(\sqrt{n})$ for appropriate $\epsilon,
  \alpha$ choice, or $\tilde{\Omega}(n^{(\log_2 (3 - \epsilon))/3})$ for any
  constant $\epsilon >0$. 

  \paragraph{Bounds on Amplification Lemma Improvement}
  Intuitively we should be able to keep growing $f_m$ until the Amplification
  Lemma stops increasing $f_m$, i.e. until $f_{m+1}= f_m$. If the optimal
  function is $n^\alpha$, then we can determine $\alpha$ in terms of $\delta$:
  $$\sum_{i\ge 0} (1-\delta)(n(1-\delta)\delta^i)^\alpha = n^\alpha (1-\delta)
  \frac{(1-\delta)^\alpha}{1-\delta^\alpha}.$$
  In order for this to be $n^\alpha$ we need
  $(1-\delta)^{\alpha+1}/(1-\delta^\alpha) = 1$, i.e. $$(1-\delta)^{\alpha+1} +
  \delta^\alpha= 1.$$
  We claim that by chosing $\delta$ to be a constant arbitrarily close to $0$,
  one can achieve exponent $\alpha$ arbitrarily close to $1$.
  Let $\epsilon$ be constant, we aim to achieve $\alpha = 1-\epsilon$.
  Consider the function $g(\delta) = (1-\delta)^{\alpha+1}+\delta^\alpha.$

  We aim to prove that there is a solution $g(\delta) = 1$ for $\delta\in (0,1)$.
  We will argue by the Intermediate Value Theorem.
  First we find a $\delta$ very close to $0$ where $g(\delta) > 1$, then we
  will find a $\delta$ where $g(\delta) < 1$. As $g(\delta)$ is clearly
  continuous, so combining these facts gives that $g(\delta) = 1$ has a solution
  on the desired interval.

  Consider the Taylor series for $(1-\delta)^{\alpha}$, $(1-\delta)^{\alpha} =
  1 - (\alpha+1)\delta - O(\delta^2).$ 
  By taking $\delta$ sufficiently small the $O(\delta^2)$ term becomes
  negligable compared to the $(\alpha+1)\delta$ term. In particular, say that
  the $O(\delta^2)$ term is less than $c \delta^2$ for some constant $c$.
  Taking $\delta$ small enough such that $\delta^2 c < \delta$, we have that
  $(1-\delta)^{\alpha} > 1-(\alpha+1)\delta - \delta$.
 
  So, to find a $\delta$ where $g(\delta) >1$ it suffices to find a $\delta$ with 
  $$\delta^\alpha \ge (\alpha+2)\delta.$$
  The equality is achieved at $\delta = (\alpha+2)^{1/(\alpha - 1)} < 1$.

  To get a $\delta$ where $g(\delta) < 1$, note that if $\alpha \ge 0.9, g(1/2)
  < 1$ which follows from the following calculation:
  $$(1/2)^{\alpha+1}+(1/2)^\alpha = \frac{3}{2}\frac{1}{2^\alpha} \le
  \frac{3}{2} \frac{1}{2^{0.9}} \approx 0.804 <1.$$

  Thus, as desired, by the Intermediate Value Theorem we have a $\delta
  \in(0,1)$ which achieves $g(\delta) = 1$.

  This argument merely establishes that $\Omega(n^{1-\epsilon})$ is the best
  backlog we can hope to attain by use of the Amplification Lemma, it does not
  guarantee that this backlog is achievable. We now show that
  the equality case of the Amplification Lemma is achievable.

  \paragraph{Achieving backlog $\Omega(n^{1-\epsilon})$}
  Recall the recursive procedure that we use in the proof of the Amplification
  Lemma: to achieve the desired fill we must call $f(n(1-\delta)\delta^i)$ for
  $\delta = 0,1,2,\ldots$. As $f_{m+1}$ recursively calls $f_m$, there is even
  more recursion.

  Let $\#(m, i)$ denote the number of times ${f_m(n(1-\delta)\delta^i)}$ occurs
  in the recurisve construction. Let there be $M = \frac{\lg (n(1-\delta))}{\lg
  (1/\delta)}$  levels of recursion. The first level in the tree has $\#(M,
  i)=1$ for all $i$. Note that we have $$\#(m-1, i) = \sum_{j > i} \#(m, j)$$
  for any level $m$, because any expression $f_m(n(1-\delta)\delta^j)$ will
  call $f_{m-1}(n(1-\delta)\delta^i)$ for $j>i$.

  This is very reminiscent of the hockey stick identity:
  $${n \choose i} = \sum_{i-1\le j\le n-1} {j \choose i-1}.$$

  In fact we claim that if you look at it right (i.e. sideways) the $\#(m,
  i)$'s form Pascal's triangle!
  Specifically the bijection is 
  $$\#(m,i) = {i \choose M-m}.$$

  This is true because of the Hockey Stick Identity and the base case
  like $\#(M, i)=1$ for all $i$. We induct on the diagonals of Pascal's
  triangle. The inductive hypothesis is that $\#(m, i) = {i \choose M-m}$ for
  all $i$ for some $m$. Then by the Hockey Stick Identity we get 
  \begin{align*}
  &\#(m-1, i) = \sum_{j>i} \#(m,j) \\
  &= \sum_{j>i} {j \choose M-m} = {i \choose M-(m-1)}
  \end{align*}
  as desired.

  We know that $f_m(n(1-\delta)\delta^M) \ge 1$ by design in Lemma
  \ref{lem:adaptiveAmplification}, so to determine the total backlog we add up
  the occurences of $f_m(n(1-\delta)\delta^M)$ on each level, weighted by the
  $\delta$ decay factor. Then the backlog we get is $$\sum_{i=0}^M {M \choose
  i}\delta^i  = (1+\delta)^{M}.$$

  Say we desire backlog $n^{1-\epsilon}$. We now solve for an appropriate
  choice of $\delta$ to yield this desired backlog.

  \begin{align*}
    (1+\delta)^M &\\
                 &= (1+\delta)^{\frac{\lg ((1-\delta)n)}{\lg (1/\delta)}}\\
    &= [(1+\delta)^{\lg (1-\delta) + \lg n)}]^{1/\lg (1/\delta)} \\
    &= n^{\frac{\lg (1+\delta)}{\lg (1/\delta)}} \cdot (1+\delta)^{\frac{\lg (1-\delta)}{\lg (1/\delta)}}.
  \end{align*}
  Thus we want 
  $$\frac{\lg(1+\delta)}{\lg(1/\delta)} = 1-\epsilon.$$
  Solving, 
  \begin{align*}
    2^{\lg(\delta+1)} = (2^{\lg(1/\delta)})^{1-\epsilon}, \\
    \iff (\delta+1)\delta^{1-\epsilon} = 1.
  \end{align*}

  Consider the function $g(\delta) = \delta^{2-\epsilon} + \delta^{1-\epsilon}$ for $\delta \in (0,1)$.
  Note that $g(0) = 0, g(1) = 2$ so by the Intermediate Value Theorem (note that $g$ is continuous, so this applies) there exists a $\delta \in (0,1)$ with $g(\delta) = 1$ as desired.

  Hence we have backlog $\Omega(n^{1-\epsilon})$ for any constant $\epsilon \in (0,1)$.


\end{proof}

\section{Oblivious Lower Bounds}\label{sec:oblivious}

An important theorem that we use repeatedly in our analysis is Hoeffding's Inequality:
\begin{theorem}[Hoeffding's Inequality]
  Let $X_i$ be independent bounded random variables with $X_i \in [a,b]$. Then,
  $$P\left(\Big|\frac{1}{n} \sum_{i=1}^n (X_i - \E[X_i])\Big|\ge t\right) \le 2\exp\left(-\frac{2nt^2}{(b-a)^2}\right) $$
\end{theorem}
Hoeffding also proved that this is true even if $X_i$ are drawn without
replacement from some finite population. This is intuitive as drawing without
replacement clearly has less variance than sampling with replacement, i.e.
sampling without replacement should be more tightly concentrated around the
mean than sampling with replacement. This is a corollary of his Theorem 4, see
page 28 of his seminal work \cite{who62}.

Call an emptying strategy $(\ell, \delta)$\defn{-greedy-like} if is satisfies
the following property when the number of processors is $p$: for any cup
$i$, if $\fil(i) + \delta > \ell$, and there are at least $p$ cups containing fill
greater than $\fil(i) + \delta$, the emptier does not empty from cup $i$.
Of particular interest is the smoothed greedy emptier, which is $(1, 0)$-greedy-like.

\begin{proposition}
  \label{prop:obliviousBase}
  There exists an oblivious filling strategy in the variable-processor cup game
  on $n$ cups that achieves backlog $\Omega(\log n)$ against a $(\ell,
  \delta)$-greedy-like emptier (where $\ell, \delta \le O(1)$ are constants
  known to the filler), with probability at least $1-1/\polylog(n)$.
\end{proposition}
\begin{proof}
  Let $A$, the \defn{anchor} set, be a subset of the cups chosen uniformly at
  random from all subsets of size $n/2$ of the cups, and let $B$, the
  \defn{non-anchor} set, consist of the rest of the cups ($|B| = n/2$). 
  Let $h = 2 \ell + g $ where $g$ is a sufficiently large constant. At each
  level of our recursive procedure we will achieve fill $h$ in some fraction of
  the cups in $A$, and because the emptier is greedy, we can turn this into a
  known set of cups with fill at least $h' = \ell + (g+\delta)/2$.
  Our strategy to achieve backlog $\Omega(\log n)$ overall is roughly as follows:
  \begin{itemize}
    \item \textbf{Step 1:} 
      Obtain large positive tilt in $B$, either by repeatedly making cups in $B$ have a constant probability of having
      fill at least $h$ and then transferring these cups into $A$, or by
      exploiting high expected positive tilt.
  \item \textbf{Step 2:} Reduce the number of processors to a constant fraction $nc$ of $n$ and
    raise the fill of $nc$ cups to $h'$. This step relies on the emptier being
    greedy.
  \item \textbf{Step 3:} Recurse on the $nc$ cups that are known to have fill at least $h'$.
\end{itemize}
We can perform $\Omega(\log n)$ levels of recursion, achieving constant backlog
at each step (relative to average fill); doing so yields backlog $\Omega(\log
n)$.

Now we detail how to achieve Step 1.
For each anchor cup $i$ we will perform a \defn{switching-process}.
First we choose an index $j \in [n^2]$; the process proceeds for $n^2$
\defn{rounds}, $j$ is the index of the switching-process at which we will
switch a cup into the anchor set.
On each of the $n^2$ rounds, the filler selects a random subset $C\subset B$ of
the non-anchor cups and plays a single processor cup game on $C$.
On most rounds, all rounds except the $j$-th the filler does nothing with the
cup that it achieves at the end of the single processor cup game.
On round $j$ with $1/2$ probability the filler swaps the winner of the single processor
cup game into the anchor set, and with $1/2$ probability the filler swaps a random cup
from $B$ into the anchor set.

We say that a cup is \defn{overpowered} if it contains fill $\ge
\sqrt{\frac{nh}{\log\log n}}$. If there is ever an overpowered cup, then the
proposition is trivially satisfied. Note that we don't need to know which cup
is overpowered because it will take $\Omega(\poly(n))$ rounds for the emptier
to reduce the fill below $\poly(n)$. Hence, we can assume without loss
of generality that no cup is ever overpowered.

We consider two cases:
\begin{itemize}
  \item \textbf{Case 1:} For at least $1/2$ of the switching-processes, at
    least $1/2$ of the cups $i \in B$ have $\fil(i) \ge -h$.
  \item \textbf{Case 2:} For at least $1/2$ of the switching-processes, less
    than $1/2$ of the cups $i \in B$ have $\fil(i) \ge -h$.
\end{itemize}

\begin{clm}
  \label{clm:reg} In Case 1, with probability at least $1-e^{-\Omega(n)}$, we
  achieve fill at least $h$ in a constant fraction of the cups in $A$, which in
  particular implies that we can achieve positive tilt $nhk$ for some known
  constant $k \in (0,1)$ ($k$ is a complicated function of $h$).
\end{clm}
\begin{proof}
  Consider a switching-process where at least $1/2$ of the cups $i \in B$
  have $\fil(i) \ge -h$.

  Say the emptier \defn{neglects} the anchor set in a round if on at least one
  step of the round the emptier does not empty from every anchor cup. By
  playing the single-processor cup game for $n^2$ rounds, with only one round
  when we actually swap a cup into the anchor set, we strongly disincentives
  the emptier from neglecting the anchor set on more than a constant fraction
  of the rounds. 

  The emptier must have some binary function, $I(k)$ that indicates whether or
  not they will neglect the anchor set on round $k$ if the filler has not already
  swapped. Note that the emptier will know when the filler perform a swap, so
  whether or not the emptier neglects a round $k$ depends on this information.
  This is the only relevant statistic that the emptier can use to decide
  whether or not to neglect a round, because on any round when we simply
  redistribute water amongst the non-anchor cups we effectively have not
  changed anything about the game state. 

  If the emptier is willing to neglect the anchor set for at least $1/2$ of the
  rounds, i.e. $\sum_{k=1}^{n^2} I(k) \ge n^2 / 2$, then with probability at
  least $1/4$, $j \in ((3/4) n^2, n^2)$, in which case the emptier neglects the anchor set
  on at least $n^2/4$ rounds ($I(k)$ must be $1$ for at least $n^2/4$ of the
  first $(3/4)n^2$ rounds). Each time the emptier neglects the anchor set the
  mass of the anchor set increases by at least $1$. Thus the average fill of the anchor
  set will have increased by at least $(n^2/2)/(n/2) \ge \Omega(\poly(n))$ over the
  entire process in this case, implying that we  win automatically as there
  must be an overpowered cup. 

  Otherwise, there is at least a $1/2$ chance that the round $j$, which is
  chosen uniformly at random from the rounds, when the filler performs a switch
  into the anchor set occurs on a round with $I(j)=0$, indicating that the emptier
  won't neglect the anchor set on round $j$. In this case, the round was a
  legitimate single processor cup game on $C_j$, the randomly chosen set of
  $e^{2h}$ cups on the $j$-th round. Then we achieve fill increase $\ge 2h$ by the
  end of the game with probability at least $1/e^{2h}!$, the probability that we
  correctly guess the sequence of cups within the single processor cup game
  that the emptier empties from. 

  The probability that the random set $C_j \subset B$ contains only elements
  with fill $\ge -h$ is basically $1/2^{e^{2h}}$, because at least half of the
  elements of $B$ have fill $\ge -h$ ({\color{red}in reality the selection of
    elements of $C$ are not independent events, but as $h$ is constant here this
  does not matter}). If all elements of $C_j$ have fill $\ge -h$, then the fill
  of the winner of the cup game has fill at least $-h + 2h = h$ if we guess the
  emptier's emptying sequence correctly.

  Combining the results, we have that for such a switching-process there is a
  constant probability of the cup which we switch into the anchor set has fill
  $\ge h$. 

  Say that this probability is $k \in (0,1)$. Then the expectation of the
  number of cups $i \in A$ with $\fil(i) \ge h$ is at least $kn/2$. Let $X_i$
  be the binary random variables, with $X_i$ taking value $1$ if the $i$-th
  switching-process succeeded, and $0$ if it failed. Then by a Chernoff Bound
  (Hoeffding's Inequality applied to Binary Random Variables),
  $$P\left(\sum_{i=1}^{n/2} X_i\le nk/4\right) \le e^{-n(k/2)^2}.$$ 
  That is, the probability that less than $nk/4$ of the anchor cups have fill
  at least $h$ is exponentially small in $n$.

\end{proof}

\begin{clm}
  \label{clm:xtreme}
  In Case 2, with probability at least $1- 1/\polylog(n)$, we achieve positive tilt $hn/8$ in the anchor set.
\end{clm}

\begin{proof}
  Consider a switching-process where we have less than $1/2$ of the cups $i\in B$
  with $\fil(i) \ge -h$.

  % RIP this totally doesn't take into account that B might start with neg fill.

  %! oh crap and fill is sinking! make sure it doesn't sink too much!!!
  We assume for simplicity that the average fill of $B$ is $0$. In reality this
  is not the case, but by a Hoeffding bound and the fact that overpowered cups don't
  exist, the fill is really tightly concentrated around $0$, so this is almost
  without loss of generality.

  Let the positive tilt of a cup $i$ be $\tilt_+(i) \defeq \max(\fil(i), 0)$.
  We have
  $$\E[\tilt_+(X)] = \frac{1}{2}\E[|\fil(X)|] \ge h$$
  (because negative tilt is at least $nh/2$ and positive tilt must oppose this).
  
  Let $Y_i$ be the random variable $Y_i=\tilt_+(X)$ where $X$ is a randomly
  selected cup from the non-anchor set at the start of the $i$-th round of
  playing single processor cups games. {\color{red}Note that the $Y_i$ are not really
  independent, but it is probably ok}. Note that $0\le Y_i \le hn/\lg\lg n$.
  Now we have, by Hoeffding's inequality, that 
  $$P\left(\Big|\frac{1}{n/2} \sum_{i=1}^{n/2} (Y_i - \E[Y_i])\Big|\ge h/2
  \right) \le$$
  $$2\exp\left(-\frac{n(h/2)^2}{(\sqrt{hn/\lg\lg n})^2}\right) $$
  $$P\left(\frac{1}{n/2}\sum_{i=1}^{n/2} Y_i \le h/4\right) \le 1/\polylog(n) $$

\end{proof}

  In both cases we achieve, with probability at least $1-1/\polylog n$,
  positive tilt at least $hnk$ in the anchor set for some known $k\in(0,1)$. Using the
  positive tilt, with one processors, we can transfer over the fill into $nk$ cups. 
  Note, we use one processor because we do not know how many cups the fill is
  concentrated in. The filler repeatedly distributes $1$ unit of fill to each
  of the $nk$ cups in succession, and continues until $h'$ fill has been
  distributed. We cannot continue beyond this point because we have used up the
  positive tilt. Now we recurse on this set of $nk$ cups.

  Note that this is the only part of this proof that was specific to a greedy
  emptier: when we wanted to achieve known fill in some cups. Against an
  arbitrary opponent we can't assume that just because they are far behind
  means that they won't oppose our attempts to achieve cups with known fill.
  Extending this result to non-greedy emptiers, or showing that it cannot be
  extended is an important open question.

  We can perform $\Omega(\log n)$ levels of recursion, and gain $\Omega(1)$
  fill at each step. Hence, overall, backlog of $\Omega(\log n)$ is achieved.
\end{proof}


\begin{lemma}[The Oblivious Amplification Lemma]
  Given an oblivious filling strategy for achieving backlog $f(n)$ in the
  variable-processor cup game on $n$ cups that succeeds with probability at
  least $1/2$, there exists a strategy for achieving ``amplified" fill 
  $$f'(n) \ge \frac{1}{32}(f(n/2) + f(n/4) + f(n/8) + \cdots) $$ that succeeds with constant probability.
\end{lemma}
\begin{proof}
  We essentially perform the same proof as Proposition \ref{prop:obliviousBase}, but some new issues arise, which we proceed to highlight and address. 

\begin{clm}
  Let a cup be \defn{verysad} if it has fill $< -nh/\lg\lg n$.
  WLOG there are no verysad cups. 
\end{clm}
\begin{proof}
  First note that because WLOG there are no overpowered cups, there fewer than $n/2$ verysad cups.

  Consider 2 cases:
  \begin{itemize}
    \item If the mass of the verysad cups is less than $nh/8$ then we can
      ignore them and accept a $-h/8$ penalty to the average fill.
    \item On the other hand, if the mass of the verysad cups is greater than
      $nh/8$, then by the end the average fill of everything else is already
      $h/8$ which is also basically as desired.
  \end{itemize}
\end{proof}

\begin{clm}
  WLOG $A,B$ have average fill $\ge -h/8$.
  In particular, we can construct a subset of $n/2$
  cups with average fill $\ge -h/8$ with high probability in $n$. 
\end{clm}
\begin{proof}

  Recall the definition of an overpowered cup as a cup with fill $\ge nh / \lg \lg n$,
  and the fact that WLOG there are no overpowered cups.
  So, If we randomly pick $B$ then this means that we are pretty good. 
  Formalizing this, let $X_i$ be the fill of the $n/2$-th randomly chosen cup
  for $B$. Unfortunately these are not quite independent events.

  Lets say we pick $2n$ things from $n$ things with replacement. Claim: with
  exponentially good probability we have $n/2$ distinct things. 
  Proof: chernoff bound. Let $X_i$ be indicator variable for cup $i$ (whether
  it was chosen or not). Probability that $X_i$ was chosen: $1-((n-1)/n)^n
  \approx 1-1/e > 1/2$ for large $n$. 
  Then by a Chernoff Bound we have that $\sum_i X_i$ is tightly concentrated
  around its mean, which is larger than $n$. In particular, with probability
  exponentially close to $1$ in $n$ we have that at least $n/2$ cups were chosen.

    initially solution: no overpowered cups wlog, so if we pick them randomly star holds
    by Hoeffding's. (kinda, bc stuff isnt really independent, can probably swap
    with replacement to fix this tho)
  
\end{proof}
\begin{clm}
  What if $C$ needs to be big because we need big backlog? 
\end{clm}
\begin{proof}
 this isn't a problem because the base case is the only case that needs to
 explicitly deal with positive and negative fill.
\end{proof}
These concerns resolved, the exact same argument as in Proposition
\ref{prop:obliviousBase} gives the desired result.

\end{proof}

\begin{corollary}
  There is an oblivious filling strategy for the variable-processor cup game on
  $n$ cups that achieves backlog $2^{\Omega(\sqrt{\log n})}$ in running time
  $O(n)$
\end{corollary}
\begin{proof}
  We must reduce want to reduce $\log^2 n$ to $\log n$ to achieve the
  appropriate running-time, so we reduce $n$ to $n' = 2^{\sqrt{\log n}}$. This
  detail taken care of we apply exactly the same recursive construction of
  $f_{\theta(\log n)}$ as in Corollary \ref{cor:adaptivePoly}, but using
  repeated application of the Oblivious Amplification Lemma rather than the
  Adaptive Amplification Lemma, which yields the disclaimer that the backlog is
  only achieved with constant probability.
  So we achieve backlog $\Omega(2^{\log n'})$ in running time $O(2^{\log^2
  n'})$. By design, expressing this in terms of $n$ we have running time $O(n)$
  (randomized lower bounds are not supposed to take longer than $\poly(n)$
  time), and as a consequence we get backlog $\Omega(2^{\sqrt{\log n}})$.
\end{proof}


\bibliographystyle{plain}
\bibliography{paper}
\end{document}
