Finally we prove that an oblivious filler can achieve backlog
$n^{1-\varepsilon}$, just like an adaptive filler despite the
oblivious filler's disadvantage. The proof is very similar to the
proof of \cref{thm:adaptivePoly}, but more complicated because in
the oblivious case we must guarantee that the result holds with
good probability, and also more complicated because the Oblivious
Amplification Lemma is more complicated than the Adaptive
Amplification Lemma. 
\begin{theorem}
  \label{thm:obliviousPoly}
  There is an oblivious filling strategy for the
  variable-processor cup game on $n$ cups that achieves backlog
  at least $\Omega(n^{1-\varepsilon})$ for any constant $\varepsilon
  >0$ in running time $2^{O(\log^2 n)}$ with probability at least
  $1-2^{-\Omega(n)}$ against any $\Delta$-greedy-like emptier for
  $\Delta \le O(1)$.
\end{theorem}
\begin{proof}
  We aim to achieve backlog $(n/n_b)^{1-\varepsilon}-1$ for some constant $n_b$ on $n$ cups.
  Let $\delta$ be a constant, chosen as a function of $\epsilon$.

  By \cref{prop:obliviousBase} there is an oblivious filling
  strategy that achieves backlog $\Omega(1)$ on $n$ cups with
  exponentially good probability in $n$; we call this algorithm
  $\alg{f_0}$. However, unlike in the proof of
  \cref{thm:adaptivePoly}, we obviously cannot use the base case
  with a constant number of cups: doing so would completely
  destroy our probability of success. Because the running time of
  our algorithm will be $2^{\polylog(n)}$, we will be required to
  take a union bound over $2^{\polylog(n)}$ events. By making the
  size of our base case $n_b = \polylog(n)$ we get that the
  probability of the algorithm failing in the base case is at
  most $2^{-\polylog(n)}$. Then, taking a union bound over
  $2^{\polylog(n)}$ events can give us the desired probability of
  success. By \cref{prop:obliviousBase} $\alg{f_0}$ achieves
  backlog $f_0(k) \ge H \ge \Omega(1)$ for all $k \ge n_b$, for
  some constant $H \ge \Omega(1)$ to be determined ($H$ is a
  function of $\delta$).

  Then we construct $f_{i+1}$ as the amplification of $f_i$ using
  \cref{lem:obliviousAmplification}.

  Define a sequence $g_i$ as 
  $$g_i =
  \begin{cases}
    n_b\ceil{16/\delta}, & i=0\\
    \floor{ g_{i-1}/(1-\delta) }, & i\ge 1 
  \end{cases}.$$

  We claim the following regarding our construction:
  \begin{clm}
    \label{clm:fikinductionagain}
    \begin{equation}
      f_i(k) \ge (k/n_b)^{1-\varepsilon} - 1 \text{ for all } k \le g_i. \label{eqn:fikinductionAGAIN}
    \end{equation}
  \end{clm}
  \begin{proof}
  We prove \cref{clm:fikinductionagain} by induction on $i$. 

  When $i=0$, the base case of our induction,
  \eqref{eqn:fikinductionAGAIN} is trivially true as
  $(k/n_b)^{1-\epsilon} - 1 \le H$ by definition of $H$ for $k\le g_0$.

  Assume \eqref{eqn:fikinductionAGAIN} for $f_i$, consider $f_{i+1}$. 

  Note that, by design of $g_i$, if $k \le g_{i+1}$ then $\floor{k\cdot (1-\delta)} \le g_i$.
  Consider any $k\in [g_{i+1}]$. 

  First we deal with the trivial
  case where $k \le g_0$. In this case
  $$f_{i+1}(k) \ge f_i(k) \ge \cdots \ge f_0(k) \ge (k/n_b)^{1-\varepsilon} -1.$$

  Now we consider $k \ge g_0$. Note that in this case $\floor{(1-\delta)k} \ge n_b$.
  Since $f_{i+1}$ is the amplification of $f_i$, and $k$ is sufficiently large, we have by \cref{lem:obliviousAmplification} that
  $$f_{i+1}(k) \ge (1-\delta)^2 f_i(\floor{(1-\delta)k}) + f_i(\ceil{\delta k}).$$
  By our inductive hypothesis, which applies as $\ceil{\delta k}\le g_i, \floor{k\cdot (1-\delta)} \le g_i$, we have
  $$f_{i+1}(k) \ge (1-\delta)^2 (\floor{(1-\delta)k/n_b}^{1-\varepsilon}-1) + \ceil{\delta k/n_b}^{1-\varepsilon} - 1. $$
  Dropping the floor and ceiling, incurring a $-1$ for dropping the floor, we have
  $$f_{i+1}(k) \ge (1-\delta)^2 (((1-\delta)k/n_b-1)^{1-\varepsilon}-1) + (\delta k/n_b)^{1-\varepsilon} - 1.$$
  Because $(x-1)^{1-\varepsilon} \ge x^{1-\varepsilon} -1$, due to the
  fact that $x\mapsto x^{1-\varepsilon}$ is a sub-linear
  sub-additive function, we have 
  $$f_{i+1}(k) \ge (1-\delta)^2 (((1-\delta)k/n_b)^{1-\varepsilon}-2) + (\delta k/n_b)^{1-\varepsilon}-1.$$
  Moving the $(k/n_b)^{1-\varepsilon}$ to the front we have
  $$ f_{i+1}(k) \ge (k/n_b)^{1-\varepsilon} \cdot\left((1-\delta)^{3-\varepsilon} + \delta^{1-\varepsilon} - \frac{2(1-\delta)^2}{(k/n_b)^{1-\varepsilon}} \right) -1.$$
  Because $(1-\delta)^{3-\varepsilon} \ge 1-(3-\varepsilon)\delta$, a fact called Bernoulli's Identity, we have
  $$f_{i+1}(k) \ge (k/n_b)^{1-\varepsilon} \cdot\left(1-(3-\varepsilon)\delta + \delta^{1-\varepsilon} - \frac{2(1-\delta)^2}{(k/n_b)^{1-\varepsilon}} \right)-1.$$
  Of course $-2(1-\delta)^2 > -2$, so 
  $$f_{i+1}(k) \ge (k/n_b)^{1-\varepsilon} \cdot\left(1-(3-\varepsilon)\delta + \delta^{1-\varepsilon} - 2/(k/n_b)^{1-\varepsilon} \right) -1.$$
  Because $$\frac{-2}{(k/n_b)^{1-\varepsilon}} \ge \frac{-2}{(g_0/n_b)^{1-\varepsilon}} \ge -2(\delta/16)^{1-\varepsilon} \ge -\delta^{1-\varepsilon}/2,$$
  which follows from our choice of $g_0 = \ceil{8/\delta} n_b$ and the restriction
  $\varepsilon<1/2$, we have 
  $$f_{i+1}(k) \ge (k/n_b)^{1-\varepsilon} \cdot\left(1-(3-\varepsilon)\delta + \delta^{1-\varepsilon} - \delta^{1-\varepsilon}/2 \right)-1.$$
  Finally, combining terms we have
  $$f_{i+1}(k) \ge  (k/n_b)^{1-\varepsilon} \cdot\left(1-(3-\varepsilon)\delta + \delta^{1-\varepsilon}/2\right)-1. $$

  Because $\delta^{1-\varepsilon}$ dominates $\delta$ for
  sufficiently small $\delta$, there is a choice of
  $\delta=\Theta(1)$ such that 
  $$1-(3-\varepsilon)\delta + \delta^{1-\varepsilon}/2 \ge 1.$$ 
  Taking $\delta$ to be this small we have,
  $$f_{i+1}(k) \ge (k/n_b)^{1-\varepsilon}-1,$$
  completing the proof. 
  \end{proof}

  The sequence $g_i$ is $n_b$ times the sequence $g_i$ from
  the proof of \cref{thm:adaptivePoly}; we thus have that $g_{i_*}
  \ge n$ for some $i_* \le O(\log n)$.
  Hence $\alg{f_{i_*}}$ achieves backlog 
  $$f_{i_*}(n) \ge (n/n_b)^{1-\varepsilon}-1.$$
  As $n_b \le \polylog(n)$ we have
  $$f_{i_*}(n) \ge \Omega(n^{1-\varepsilon}),$$ as desired.

  Let the running time of $f_i(n)$ be $T_i(n)$. From the Amplification Lemma we have following recurrence bounding $T_i(n)$:
  \begin{align*}
    T_i(n) &\le 6n^{\eta+1} \delta \cdot T_{i-1}(\floor{(1-\delta)n}) +
  T_{i-1}(\ceil{\delta n}) \\
  &\le 7n^{\eta+1}T_{i-1}(\floor{(1-\delta)n}).
  \end{align*}
  It follows that $\alg{f_{i_*}}$, recalling that $i_* \le O(\log n)$, has running time
  $$T_{i_*}(n) \le (7n^{\eta+1})^{O(\log n)} \le 2^{O(\log^2 n)}$$
  as desired.

  As noted, because the running time is $2^{\polylog(n)}$ and the
  base case size is $n_b\ge \polylog(n)$, a union bound
  guarantees the probability of success is at least
  $1-2^{-\polylog(n)}$.
\end{proof}

% Im not sure this is true anymore: [remark: I'm not super
% enthusiastic about trying to prove this; we'll see if I can get
% the argument simpler for the case $\Delta \le O(1)$, but right
% now even that is really intense, don't want to make it more
% intense for very little additional coolness benefit, I mean
% $\omega(1)$ is kinda neat, but still]
% \section{oblivious lower bound part 2}
% in fact, our results (probably) even hold for $\Delta \le O(\log \log n)$
% you just need to modify the proposition, the lemma doesn't care too much
% modify the proposition to *only need one of them to succeed*.

